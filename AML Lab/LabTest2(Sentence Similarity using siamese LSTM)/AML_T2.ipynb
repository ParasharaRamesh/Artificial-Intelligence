{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "# download this to use gensim word2vec, KyedVectors\n",
    "EMBEDDING_FILE = \"GoogleNews-vectors-negative300.bin.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           question1  \\\n",
      "0  What is the step by step guide to invest in sh...   \n",
      "1  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
      "2  How can I increase the speed of my internet co...   \n",
      "3  Why am I mentally very lonely? How can I solve...   \n",
      "4  Which one dissolve in water quikly sugar, salt...   \n",
      "\n",
      "                                           question2  is_duplicate  \n",
      "0  What is the step by step guide to invest in sh...             0  \n",
      "1  What would happen if the Indian government sto...             0  \n",
      "2  How can Internet speed be increased by hacking...             0  \n",
      "3  Find the remainder when [math]23^{24}[/math] i...             0  \n",
      "4            Which fish would survive in salt water?             0  \n"
     ]
    }
   ],
   "source": [
    "# load the datset, keep the columns we want\n",
    "NUM_ROWS = 5000\n",
    "data_csv = pd.read_csv(\"questions.csv\")\n",
    "data_csv = data_csv[['question1','question2','is_duplicate']].head(NUM_ROWS) # change to higher number on faster machine\n",
    "questions_cols = ['question1', 'question2']\n",
    "print(data_csv.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = set(stopwords.words('english'))\n",
    "\n",
    "# refer: https://github.com/eliorc/Medium/blob/master/MaLSTM.ipynb\n",
    "\n",
    "def text_to_word_list(text):\n",
    "    ''' Pre process and convert texts to a list of words '''\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "\n",
    "    text = text.split()\n",
    "\n",
    "    return text\n",
    "vocabulary = dict()\n",
    "inverse_vocabulary = ['<unk>']  # '<unk>' will never be used, it is only a placeholder for the [0, 0, ....0] embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading word2vec\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Prepare embedding\n",
    "\n",
    "print('loading word2vec')\n",
    "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\ipykernel_launcher.py:17: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making embeddings\n"
     ]
    }
   ],
   "source": [
    "for index, row in data_csv.iterrows():\n",
    " \n",
    "    for question in questions_cols:\n",
    "        \n",
    "        q2n = []  # q2n -> question numbers representation\n",
    "        for word in text_to_word_list(row[question]):\n",
    "            if word in stops and word not in word2vec.vocab:\n",
    "                continue\n",
    "\n",
    "            if word not in vocabulary:\n",
    "                vocabulary[word] = len(inverse_vocabulary)\n",
    "                q2n.append(len(inverse_vocabulary))\n",
    "                inverse_vocabulary.append(word)\n",
    "            else:\n",
    "                q2n.append(vocabulary[word])\n",
    "\n",
    "        data_csv.set_value(index, question, q2n)\n",
    "\n",
    "print('making embeddings')\n",
    "\n",
    "embedding_dim = 300\n",
    "embeddings = 1 * np.random.randn(len(vocabulary) + 1, embedding_dim)  # This will be the embedding matrix\n",
    "embeddings[0] = 0  # So that the padding will be ignored\n",
    "\n",
    "# Build the embedding matrix\n",
    "for word, index in vocabulary.items():\n",
    "    if word in word2vec.vocab:\n",
    "        embeddings[index] = word2vec.word_vec(word)\n",
    "\n",
    "del word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max seq length:  212\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_seq_length = max(data_csv.question1.map(lambda x: len(x)).max(),\n",
    "                     data_csv.question2.map(lambda x: len(x)).max())\n",
    "\n",
    "print(\"max seq length: \", max_seq_length)\n",
    "\n",
    "# Split to train validation\n",
    "validation_size = int(0.2*NUM_ROWS)\n",
    "training_size = len(data_csv) - validation_size\n",
    "\n",
    "X = data_csv[questions_cols]\n",
    "Y = data_csv['is_duplicate']\n",
    "\n",
    "X_train, X_validation, Y_train, Y_validation = train_test_split(X, Y, test_size=validation_size)\n",
    "\n",
    "X_train = {'left': X_train.question1, 'right': X_train.question2}\n",
    "X_validation = {'left': X_validation.question1, 'right': X_validation.question2}\n",
    "\n",
    "Y_train = Y_train.values\n",
    "Y_validation = Y_validation.values\n",
    "\n",
    "for dataset, side in itertools.product([X_train, X_validation], ['left', 'right']):\n",
    "    dataset[side] = pad_sequences(dataset[side], maxlen=int(max_seq_length))\n",
    "\n",
    "\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.13964844 -0.00616455  0.21484375  0.07275391 -0.16113281  0.07568359\n",
      "  0.16796875 -0.20117188  0.12597656  0.00915527  0.05249023 -0.15136719\n",
      " -0.02758789  0.04199219 -0.234375    0.13867188 -0.02600098  0.07910156\n",
      "  0.02746582 -0.13085938 -0.02478027  0.10009766 -0.07910156 -0.07714844\n",
      "  0.03759766  0.16894531  0.05371094 -0.05200195  0.14453125 -0.04370117\n",
      " -0.12597656  0.06884766 -0.10595703 -0.14550781 -0.00331116  0.01367188\n",
      "  0.13964844  0.01660156  0.03417969  0.16113281 -0.01080322  0.06689453\n",
      "  0.06835938 -0.15136719 -0.16894531  0.03295898 -0.06884766  0.06787109\n",
      " -0.07373047  0.08300781  0.05761719  0.14550781 -0.11865234 -0.13671875\n",
      "  0.12402344  0.04296875 -0.11962891 -0.08154297  0.06494141 -0.05639648\n",
      " -0.04394531  0.1484375  -0.07714844  0.04614258 -0.02624512 -0.06591797\n",
      "  0.04980469  0.08886719 -0.01647949 -0.02294922  0.10546875  0.04199219\n",
      "  0.11035156 -0.08251953 -0.13574219 -0.07324219  0.1015625   0.05371094\n",
      " -0.07275391  0.08496094 -0.04443359 -0.078125    0.08398438 -0.00613403\n",
      " -0.20898438 -0.25        0.00485229  0.22363281  0.01550293  0.04223633\n",
      "  0.07861328  0.203125   -0.25195312  0.01867676  0.03564453 -0.09863281\n",
      "  0.01745605  0.12597656 -0.04589844 -0.10253906 -0.10742188 -0.00558472\n",
      "  0.05517578 -0.10791016 -0.1015625   0.0222168  -0.07958984  0.04833984\n",
      " -0.06201172 -0.11132812  0.16210938 -0.09716797 -0.03222656  0.08056641\n",
      "  0.21386719 -0.03759766  0.06542969 -0.15527344  0.00300598 -0.04907227\n",
      " -0.23730469  0.13378906  0.10253906  0.07568359  0.01330566 -0.02770996\n",
      " -0.27929688  0.03112793  0.00092316 -0.10107422 -0.23730469 -0.21484375\n",
      " -0.08496094 -0.16894531  0.04370117 -0.20996094  0.00100708  0.07617188\n",
      " -0.03198242  0.14160156  0.15820312 -0.01275635  0.04150391 -0.03393555\n",
      "  0.12011719 -0.08789062 -0.03735352 -0.16503906 -0.14257812 -0.05200195\n",
      "  0.06542969  0.22070312 -0.34570312  0.10400391  0.05053711 -0.02368164\n",
      " -0.13671875 -0.13476562  0.09863281  0.06689453 -0.07666016  0.20214844\n",
      " -0.01806641 -0.06201172  0.00402832 -0.04174805  0.06835938 -0.04882812\n",
      "  0.12890625  0.14941406 -0.07763672  0.09179688  0.03686523 -0.08789062\n",
      " -0.01721191  0.15625     0.16210938 -0.11328125 -0.00830078 -0.11962891\n",
      " -0.16601562 -0.12792969  0.03759766 -0.16601562  0.10449219 -0.01220703\n",
      " -0.01940918  0.10009766  0.0098877   0.05957031  0.17285156  0.1484375\n",
      "  0.21191406 -0.06835938 -0.04443359 -0.12158203  0.03088379  0.02392578\n",
      " -0.05297852 -0.09912109 -0.00375366  0.15625    -0.06884766  0.10205078\n",
      "  0.00448608  0.05053711 -0.11035156 -0.15332031  0.03808594 -0.05249023\n",
      "  0.01226807  0.08935547  0.06005859 -0.08007812 -0.24902344 -0.01953125\n",
      "  0.25390625  0.00915527 -0.04345703  0.0612793  -0.06884766  0.1015625\n",
      " -0.09326172 -0.07763672  0.15625    -0.10546875  0.0625      0.13574219\n",
      " -0.06982422  0.12792969  0.05957031 -0.14550781  0.08251953 -0.12792969\n",
      "  0.14648438 -0.15332031 -0.01708984 -0.01672363  0.07958984  0.01794434\n",
      "  0.04199219 -0.12353516  0.03320312 -0.11083984 -0.09716797 -0.07568359\n",
      "  0.14453125 -0.10351562  0.05566406  0.03369141  0.01422119  0.17382812\n",
      "  0.10595703  0.03930664  0.27539062 -0.14453125  0.01672363  0.03369141\n",
      " -0.06542969 -0.1640625   0.00909424 -0.07910156 -0.14453125  0.03979492\n",
      " -0.05761719  0.078125    0.12402344  0.00671387 -0.19140625  0.04248047\n",
      "  0.02844238  0.10351562  0.33007812  0.25       -0.14160156  0.04003906\n",
      " -0.00201416 -0.12255859 -0.05297852  0.02587891  0.11669922 -0.07861328\n",
      "  0.03320312  0.14257812 -0.02856445 -0.06494141  0.03955078 -0.07421875\n",
      " -0.07080078  0.07714844 -0.1015625  -0.08300781 -0.11767578 -0.09619141\n",
      " -0.203125   -0.02490234  0.19335938  0.05712891  0.09960938 -0.234375  ]\n"
     ]
    }
   ],
   "source": [
    "print(embeddings[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\ipykernel_launcher.py:39: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4000 samples, validate on 1000 samples\n",
      "Epoch 1/25\n",
      "4000/4000 [==============================] - 41s 10ms/step - loss: 1.2339 - acc: 0.6470 - val_loss: 1.0229 - val_acc: 0.6570\n",
      "Epoch 2/25\n",
      "4000/4000 [==============================] - 40s 10ms/step - loss: 0.7168 - acc: 0.6815 - val_loss: 0.6421 - val_acc: 0.6740\n",
      "Epoch 3/25\n",
      "4000/4000 [==============================] - 42s 10ms/step - loss: 0.5578 - acc: 0.7298 - val_loss: 0.6340 - val_acc: 0.7010\n",
      "Epoch 4/25\n",
      "4000/4000 [==============================] - 41s 10ms/step - loss: 0.5204 - acc: 0.7525 - val_loss: 0.6305 - val_acc: 0.6670\n",
      "Epoch 5/25\n",
      "4000/4000 [==============================] - 41s 10ms/step - loss: 0.4921 - acc: 0.7648 - val_loss: 0.6209 - val_acc: 0.6910\n",
      "Epoch 6/25\n",
      "4000/4000 [==============================] - 40s 10ms/step - loss: 0.4739 - acc: 0.7770 - val_loss: 0.6178 - val_acc: 0.7110\n",
      "Epoch 7/25\n",
      "4000/4000 [==============================] - 40s 10ms/step - loss: 0.4580 - acc: 0.7880 - val_loss: 0.6197 - val_acc: 0.6930\n",
      "Epoch 8/25\n",
      "4000/4000 [==============================] - 40s 10ms/step - loss: 0.4436 - acc: 0.7980 - val_loss: 0.6164 - val_acc: 0.7120\n",
      "Epoch 9/25\n",
      "4000/4000 [==============================] - 41s 10ms/step - loss: 0.4319 - acc: 0.8095 - val_loss: 0.6177 - val_acc: 0.7090\n",
      "Epoch 10/25\n",
      "4000/4000 [==============================] - 40s 10ms/step - loss: 0.4233 - acc: 0.8118 - val_loss: 0.6161 - val_acc: 0.6980\n",
      "Epoch 11/25\n",
      "4000/4000 [==============================] - 41s 10ms/step - loss: 0.4132 - acc: 0.8190 - val_loss: 0.6215 - val_acc: 0.7180\n",
      "Epoch 12/25\n",
      "4000/4000 [==============================] - 41s 10ms/step - loss: 0.4071 - acc: 0.8293 - val_loss: 0.6255 - val_acc: 0.7190\n",
      "Epoch 13/25\n",
      "4000/4000 [==============================] - 41s 10ms/step - loss: 0.3985 - acc: 0.8333 - val_loss: 0.6191 - val_acc: 0.7170\n",
      "Epoch 14/25\n",
      "4000/4000 [==============================] - 41s 10ms/step - loss: 0.3925 - acc: 0.8387 - val_loss: 0.6230 - val_acc: 0.7180\n",
      "Epoch 15/25\n",
      "4000/4000 [==============================] - 41s 10ms/step - loss: 0.3868 - acc: 0.8410 - val_loss: 0.6225 - val_acc: 0.7100\n",
      "Epoch 16/25\n",
      "4000/4000 [==============================] - 41s 10ms/step - loss: 0.3805 - acc: 0.8498 - val_loss: 0.6245 - val_acc: 0.7050\n",
      "Epoch 17/25\n",
      "4000/4000 [==============================] - 41s 10ms/step - loss: 0.3752 - acc: 0.8552 - val_loss: 0.6245 - val_acc: 0.7210\n",
      "Epoch 18/25\n",
      "4000/4000 [==============================] - 41s 10ms/step - loss: 0.3711 - acc: 0.8575 - val_loss: 0.6264 - val_acc: 0.7150\n",
      "Epoch 19/25\n",
      "4000/4000 [==============================] - 41s 10ms/step - loss: 0.3660 - acc: 0.8640 - val_loss: 0.6349 - val_acc: 0.7130\n",
      "Epoch 20/25\n",
      "4000/4000 [==============================] - 40s 10ms/step - loss: 0.3609 - acc: 0.8640 - val_loss: 0.6304 - val_acc: 0.7170\n",
      "Epoch 21/25\n",
      "4000/4000 [==============================] - 41s 10ms/step - loss: 0.3572 - acc: 0.8708 - val_loss: 0.6325 - val_acc: 0.7190\n",
      "Epoch 22/25\n",
      "4000/4000 [==============================] - 41s 10ms/step - loss: 0.3529 - acc: 0.8748 - val_loss: 0.6367 - val_acc: 0.7180\n",
      "Epoch 23/25\n",
      "4000/4000 [==============================] - 41s 10ms/step - loss: 0.3486 - acc: 0.8768 - val_loss: 0.6358 - val_acc: 0.7120\n",
      "Epoch 24/25\n",
      "4000/4000 [==============================] - 40s 10ms/step - loss: 0.3453 - acc: 0.8792 - val_loss: 0.6446 - val_acc: 0.7140\n",
      "Epoch 25/25\n",
      "1664/4000 [===========>..................] - ETA: 21s - loss: 0.3331 - acc: 0.8816"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, GRU, Lambda\n",
    "import keras.backend as K\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Model variables\n",
    "n_hidden = 50\n",
    "gradient_clipping_norm = 1.25\n",
    "batch_size = 64\n",
    "n_epoch = 25\n",
    "\n",
    "\n",
    "left_input = Input(shape=(max_seq_length,), dtype='float')\n",
    "right_input = Input(shape=(max_seq_length,), dtype='float')\n",
    "\n",
    "embedding_layer = Embedding(len(embeddings), embedding_dim, weights=[embeddings], input_length=max_seq_length, trainable=False)\n",
    "\n",
    "encoded_left = embedding_layer(left_input)\n",
    "encoded_right = embedding_layer(right_input)\n",
    "\n",
    "shared_gru = GRU(n_hidden)\n",
    "\n",
    "left_output = shared_gru(encoded_left)\n",
    "right_output = shared_gru(encoded_right)\n",
    "\n",
    "def exponent_neg_manhattan_distance(left, right):\n",
    "    return K.exp(-K.sum(K.abs(left-right), axis=1, keepdims=True))\n",
    "\n",
    "malstm_distance = Lambda(function=lambda x: exponent_neg_manhattan_distance(x[0], x[1]),output_shape=lambda x: (x[0][0], 1))([left_output, right_output])\n",
    "\n",
    "malstm = Model([left_input, right_input],[malstm_distance])\n",
    "\n",
    "\n",
    "malstm.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "malstm_trained = malstm.fit([X_train['left'], X_train['right']], Y_train, batch_size=batch_size, nb_epoch=n_epoch,\n",
    "                            validation_data=([X_validation['left'], X_validation['right']], Y_validation))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
